{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1d00a80e",
      "metadata": {},
      "source": [
        "# Transformer Arcihtecture"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5715bfb1",
      "metadata": {},
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43d6aaaa",
      "metadata": {},
      "source": [
        "Let Î£ be the finite set of activity labels. Traces have variable length L.\n",
        "We must embed every trace into a common sequence length ð‘‡_max.\n",
        "We extend the activity alphabet:\n",
        "\n",
        "Î£â€² = Î£ âˆª { CLS, EOS, PAD } where,\n",
        "CLS: global trace representation anchor\n",
        "EOS: semantic end of execution\n",
        "PAD: absence of data (non-semantic)\n",
        "\n",
        "Attention masking for paddings:\n",
        "if padding -> m_i = 0, otherwise m_i = 1\n",
        "\n",
        "We also add imports/installations in this section"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68f07522",
      "metadata": {},
      "source": [
        "### Initial Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58300939",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run once to install required libs\n",
        "%pip install pm4py torch pandas numpy\n",
        "\n",
        "%pip install -U ipywidgets jupyter jupyterlab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9d4c1d99",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Install and Imports Part\n",
        "\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple, Optional, Sequence\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# PM4Py\n",
        "from pm4py.objects.petri_net.importer import importer as pnml_importer\n",
        "from pm4py.algo.simulation.playout.petri_net import algorithm as pn_playout\n",
        "from pm4py.objects.log.importer.xes import importer as xes_importer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1a018ee3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device: cpu\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Configurations, path declerations, training hyperparams. are as follows:\n",
        "\n",
        "BASE_DIR = \"../Data/Experiment1\"\n",
        "EVENT_XES_PATH = os.path.join(BASE_DIR, \"Event_Log.xes\")\n",
        "\n",
        "N_MODELS = 15\n",
        "PNML_TEMPLATE = os.path.join(BASE_DIR, \"Model{}.pnml\")\n",
        "\n",
        "# PM4Py playout\n",
        "N_TRACES_PER_MODEL = 3000\n",
        "PM4PY_MAX_TRACE_LENGTH = 200\n",
        "\n",
        "# Training\n",
        "SEED = 42\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 20\n",
        "LR = 3e-4\n",
        "WEIGHT_DECAY = 0.0\n",
        "\n",
        "# Transformer\n",
        "D_MODEL = 128\n",
        "N_LAYERS = 2\n",
        "N_HEADS = 4\n",
        "D_FF = 512\n",
        "DROPOUT = 0.0\n",
        "\n",
        "# Dynamic padding (batch-by-batch)\n",
        "# Here, positional encoding length must be >= longest tokenized trace length in a batch (thus, dynamic).\n",
        "POSENC_MAXLEN = 512 # but we still have to limit the positional encodings\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(\"device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "01c671db",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Reproducibility seed:\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a48b8e66",
      "metadata": {},
      "source": [
        "### Token Declerations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e345d5a0",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Special Tokens + Vocabulary is defined as follows:\n",
        "SPECIAL_TOKENS = {\n",
        "    \"PAD\": \"[PAD]\",\n",
        "    \"CLS\": \"[CLS]\",\n",
        "    \"EOS\": \"[EOS]\",\n",
        "    \"UNK\": \"[UNK]\",\n",
        "}\n",
        "\n",
        "def build_vocab(traces: List[List[str]], min_freq: int = 1) -> Tuple[Dict[str,int], Dict[int,str]]:\n",
        "    from collections import Counter\n",
        "    c = Counter(a for t in traces for a in t)\n",
        "\n",
        "    stoi = {\n",
        "        SPECIAL_TOKENS[\"PAD\"]: 0,\n",
        "        SPECIAL_TOKENS[\"CLS\"]: 1,\n",
        "        SPECIAL_TOKENS[\"EOS\"]: 2,\n",
        "        SPECIAL_TOKENS[\"UNK\"]: 3,\n",
        "    }\n",
        "    itos = {i: s for s, i in stoi.items()}\n",
        "\n",
        "    for act, freq in c.items():\n",
        "        if freq >= min_freq and act not in stoi:\n",
        "            idx = len(stoi)\n",
        "            stoi[act] = idx\n",
        "            itos[idx] = act\n",
        "\n",
        "    return stoi, itos"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8645680",
      "metadata": {},
      "source": [
        "### Reading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "992715e8",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Loading: XES \n",
        "# Play out the models\n",
        "\n",
        "def event_log_to_traces(log, activity_key: str = \"concept:name\") -> List[List[str]]:\n",
        "    traces = []\n",
        "    for tr in log:\n",
        "        seq = [str(ev[activity_key]) for ev in tr if activity_key in ev]\n",
        "        if seq:\n",
        "            traces.append(seq)\n",
        "    return traces\n",
        "\n",
        "def load_xes_traces(xes_path: str, activity_key: str = \"concept:name\") -> List[List[str]]:\n",
        "    log = xes_importer.apply(xes_path)\n",
        "    return event_log_to_traces(log, activity_key=activity_key)\n",
        "\n",
        "def load_pnml(pnml_path: str):\n",
        "    net, im, fm = pnml_importer.apply(pnml_path)\n",
        "    return net, im, fm\n",
        "\n",
        "def playout_to_event_log(net, im, fm, n_traces: int, max_trace_length: int):\n",
        "    params = {\n",
        "        pn_playout.Variants.BASIC_PLAYOUT.value.Parameters.NO_TRACES: n_traces,\n",
        "        pn_playout.Variants.BASIC_PLAYOUT.value.Parameters.MAX_TRACE_LENGTH: max_trace_length,\n",
        "    }\n",
        "    return pn_playout.apply(net, im, fm, variant=pn_playout.Variants.BASIC_PLAYOUT, parameters=params)\n",
        "\n",
        "def playout_pnml_to_traces(pnml_path: str, n_traces: int, max_trace_length: int) -> List[List[str]]:\n",
        "    net, im, fm = load_pnml(pnml_path)\n",
        "    log = playout_to_event_log(net, im, fm, n_traces=n_traces, max_trace_length=max_trace_length)\n",
        "    return event_log_to_traces(log)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14f5496f",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Playout traces for each model for experiment 1 \n",
        "event_traces = load_xes_traces(EVENT_XES_PATH)\n",
        "print(\"Loaded event traces:\", len(event_traces))\n",
        "\n",
        "model_traces_by_model: Dict[str, List[List[str]]] = {}\n",
        "for i in range(1, N_MODELS + 1):\n",
        "    path = PNML_TEMPLATE.format(i)\n",
        "    model_name = f\"Model{i}\"\n",
        "    traces = playout_pnml_to_traces(path, n_traces=N_TRACES_PER_MODEL, max_trace_length=PM4PY_MAX_TRACE_LENGTH)\n",
        "    model_traces_by_model[model_name] = traces\n",
        "    print(model_name, \"traces:\", len(traces))\n",
        "\n",
        "all_model_traces = [t for traces in model_traces_by_model.values() for t in traces]\n",
        "print(\"Total model traces:\", len(all_model_traces))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6d81158",
      "metadata": {},
      "source": [
        "### Tokenization of traces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "1c18d01b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vocab_size: 13\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Build shared vocabulary\n",
        "stoi, itos = build_vocab(event_traces + all_model_traces)\n",
        "print(\"vocab_size:\", len(stoi)) # for testing if correctly done\n",
        "\n",
        "PAD_ID = stoi[SPECIAL_TOKENS[\"PAD\"]]\n",
        "CLS_ID = stoi[SPECIAL_TOKENS[\"CLS\"]]\n",
        "EOS_ID = stoi[SPECIAL_TOKENS[\"EOS\"]]\n",
        "UNK_ID = stoi[SPECIAL_TOKENS[\"UNK\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99adb641",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Tokenizing the traces \n",
        "# We add CLS + EOS. No global truncation is applied; batches are padded dynamically!!\n",
        "\n",
        "def tokenize_trace(trace: List[str], stoi: Dict[str,int]) -> List[int]:\n",
        "    ids = [CLS_ID] + [stoi.get(a, UNK_ID) for a in trace] + [EOS_ID]\n",
        "    return ids\n",
        "\n",
        "def tokenize_traces(traces: List[List[str]], stoi: Dict[str,int]) -> List[List[int]]:\n",
        "    return [tokenize_trace(t, stoi) for t in traces]\n",
        "\n",
        "event_token_traces = tokenize_traces(event_traces, stoi)\n",
        "model_token_traces_by_model = {m: tokenize_traces(trs, stoi) for m, trs in model_traces_by_model.items()}\n",
        "\n",
        "print(\"Example tokenized event trace:\", event_token_traces[0])\n",
        "print(\"POSENC_MAXLEN:\", POSENC_MAXLEN)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a55bd684",
      "metadata": {},
      "source": [
        "### Dynamic padding\n",
        "We do **dynamic padding inside each batch**, so we avoid the huge PAD ratios observed when padding everything to a large global max length. This helps the training of the classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "6a0dd474",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def collate_token_traces(batch: List[Tuple[List[int], float]]):\n",
        "    \"\"\"Pads within-batch to max length in batch.\"\"\"\n",
        "    ids_list, labels = zip(*batch)\n",
        "    max_len = max(len(x) for x in ids_list)\n",
        "    # Pad\n",
        "    input_ids = torch.full((len(ids_list), max_len), PAD_ID, dtype=torch.long)\n",
        "    attention_mask = torch.zeros((len(ids_list), max_len), dtype=torch.bool)\n",
        "    for i, ids in enumerate(ids_list):\n",
        "        input_ids[i, :len(ids)] = torch.tensor(ids, dtype=torch.long)\n",
        "        attention_mask[i, :len(ids)] = True\n",
        "    labels = torch.tensor(labels, dtype=torch.float32)\n",
        "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6eee36c9",
      "metadata": {},
      "source": [
        "### Generation and Building of antilogs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40bd9073",
      "metadata": {},
      "source": [
        "This is the negatives.\n",
        "This creates â€œplausible-looking but behaviorally implausibleâ€ traces by applying a few random corruptions:\n",
        "1- swap two activities\n",
        "2- replace some activities with random ones\n",
        "3- shuffle a contiguous segment\n",
        "4- We do not accept a trace as negative if it exists in positive log\n",
        "\n",
        "For precision checking, we check how fit our model is compared to the event log. So, we have an event log and event antilog. And we compare this classifier with our models to cehck how similar our model looks like.\n",
        "\n",
        "Precision classifier:\n",
        "Training data: positives: event log traces, negatives: event antilog traces\n",
        "What it learns: â€œWhat behavior does the model typically allow?â€\n",
        "High precision â†’ model does not allow much extra behavior\n",
        "Low precision â†’ model allows behavior not seen in reality\n",
        "\n",
        "Fitness classifier:\n",
        "Training data: positives: model log traces, negatives: model antilog traces\n",
        "What it learns: â€œWhat does real observed behavior look like?â€\n",
        "High fitness â†’ model behavior looks like real behavior\n",
        "Low fitness â†’ model misses or distorts observed behavior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7ced69b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Antilog generation (hybrid: splice + corruption)\n",
        "\n",
        "def _activity_alphabet_from_vocab(stoi: dict) -> List[str]:\n",
        "    specials = set(SPECIAL_TOKENS.values())\n",
        "    return [tok for tok in stoi.keys() if tok not in specials]\n",
        "\n",
        "def corrupt_trace(trace: List[str], alphabet: Sequence[str], rng: random.Random,\n",
        "                  p_swap: float = 0.8,\n",
        "                  p_replace: float = 0.95,\n",
        "                  p_shuffle_segment: float = 0.7,\n",
        "                  replace_rate: float = 0.5) -> List[str]:\n",
        "    if len(trace) <= 1:\n",
        "        return trace[:]\n",
        "    t = trace[:]\n",
        "\n",
        "    if rng.random() < p_swap and len(t) >= 2:\n",
        "        i, j = rng.sample(range(len(t)), 2)\n",
        "        t[i], t[j] = t[j], t[i]\n",
        "\n",
        "    if rng.random() < p_replace and len(alphabet) > 0:\n",
        "        # ensure at least 2 replacements when possible\n",
        "        k = max(1, int(round(replace_rate * len(t))))\n",
        "        if len(t) >= 4:\n",
        "            k = max(k, 2)\n",
        "        idxs = rng.sample(range(len(t)), k)\n",
        "        for idx in idxs:\n",
        "            t[idx] = rng.choice(alphabet)\n",
        "\n",
        "    if rng.random() < p_shuffle_segment and len(t) >= 4:\n",
        "        a = rng.randint(0, len(t) - 2)\n",
        "        b = rng.randint(a + 1, min(len(t), a + 1 + max(2, len(t)//2)))\n",
        "        seg = t[a:b]\n",
        "        rng.shuffle(seg)\n",
        "        t[a:b] = seg\n",
        "\n",
        "    return t\n",
        "\n",
        "def splice_negative(traces: List[List[str]], rng: random.Random) -> List[str]:\n",
        "    a = rng.choice(traces)\n",
        "    b = rng.choice(traces)\n",
        "    if len(a) < 2 or len(b) < 2:\n",
        "        return a[:] if len(a) else b[:]\n",
        "    cut_a = rng.randint(1, len(a)-1)\n",
        "    cut_b = rng.randint(1, len(b)-1)\n",
        "    return a[:cut_a] + b[cut_b:]\n",
        "\n",
        "def generate_antilog_hybrid(traces: List[List[str]], stoi: dict, n_neg: Optional[int] = None,\n",
        "                            seed: int = 42, p_splice: float = 0.4, max_attempts: int = 200000) -> List[List[str]]:\n",
        "    rng = random.Random(seed)\n",
        "    alphabet = _activity_alphabet_from_vocab(stoi)\n",
        "    if n_neg is None:\n",
        "        n_neg = len(traces)\n",
        "\n",
        "    originals = {tuple(t) for t in traces}\n",
        "    negatives = []\n",
        "    attempts = 0\n",
        "\n",
        "    while len(negatives) < n_neg and attempts < max_attempts:\n",
        "        attempts += 1\n",
        "        base = rng.choice(traces)\n",
        "\n",
        "        if rng.random() < p_splice:\n",
        "            neg = splice_negative(traces, rng)\n",
        "        else:\n",
        "            # stronger corruption for short traces\n",
        "            rr = min(0.8, max(0.5, 3 / max(1, len(base))))\n",
        "            neg = corrupt_trace(base, alphabet, rng, replace_rate=rr)\n",
        "\n",
        "        if tuple(neg) in originals:\n",
        "            continue\n",
        "        negatives.append(neg)\n",
        "\n",
        "    if len(negatives) < n_neg:\n",
        "        print(f\"Warning: generated {len(negatives)}/{n_neg} antilog traces after {attempts} attempts\")\n",
        "\n",
        "    return negatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d90cfff2",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Build antilogs \n",
        "# for event antilog once\n",
        "# model for each model\n",
        "event_antilog_traces = generate_antilog_hybrid(event_traces, stoi, n_neg=len(event_traces), seed=SEED + 100)\n",
        "model_antilog_traces_by_model = {\n",
        "    m: generate_antilog_hybrid(trs, stoi, n_neg=len(trs), seed=SEED + 200 + (hash(m) % 10000))\n",
        "    for m, trs in model_traces_by_model.items()\n",
        "}\n",
        "\n",
        "# Tokenize antilogs\n",
        "event_antilog_tokens = tokenize_traces(event_antilog_traces, stoi)\n",
        "model_antilog_tokens_by_model = {\n",
        "    m: tokenize_traces(trs, stoi) for m, trs in model_antilog_traces_by_model.items()\n",
        "}\n",
        "\n",
        "print(\"event_antilog:\", len(event_antilog_tokens))\n",
        "print(\"model_antilog example:\", len(model_antilog_tokens_by_model[\"Model1\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7279c256",
      "metadata": {},
      "source": [
        "## Architecture\n",
        "For encoder layer we have MHA + FFN\n",
        "Then, we train binary classifiers with balanced positives and negatives.\n",
        "\n",
        "**Fitness per model**: positives = model log, negatives = model antilog, score on event log  \n",
        "**Precision once**: positives = event log, negatives = event antilog, score on model logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "b7904cc8",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class TokenTraceBinaryDataset(Dataset):\n",
        "    def __init__(self, pos: List[List[int]], neg: List[List[int]], seed: int = 42, n_per_class: Optional[int] = None):\n",
        "        rng = np.random.default_rng(seed)\n",
        "        n_pos = len(pos)\n",
        "        n_neg = len(neg)\n",
        "        n = min(n_pos, n_neg) if n_per_class is None else min(n_per_class, n_pos, n_neg)\n",
        "\n",
        "        pos_idx = rng.choice(n_pos, size=n, replace=False)\n",
        "        neg_idx = rng.choice(n_neg, size=n, replace=False)\n",
        "\n",
        "        self.samples: List[Tuple[List[int], float]] = []\n",
        "        for i in pos_idx:\n",
        "            self.samples.append((pos[i], 1.0))\n",
        "        for i in neg_idx:\n",
        "            self.samples.append((neg[i], 0.0))\n",
        "\n",
        "        rng.shuffle(self.samples)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "f688e6fc",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Transformer encoder + classifier part\n",
        "\n",
        "class SinusoidalPositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, max_len: int):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model, dtype=torch.float32)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        T = x.size(1)\n",
        "        return x + self.pe[:T].unsqueeze(0)\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.mha = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_ff, d_model),\n",
        "        )\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
        "        key_padding_mask = ~attention_mask  # True for PAD\n",
        "        attn_out, _ = self.mha(x, x, x, key_padding_mask=key_padding_mask, need_weights=False)\n",
        "        x = self.norm1(x + self.dropout1(attn_out))\n",
        "        ffn_out = self.ffn(x)\n",
        "        x = self.norm2(x + self.dropout2(ffn_out))\n",
        "        return x\n",
        "\n",
        "class TraceTransformerEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size: int, max_len: int, d_model: int, n_layers: int, n_heads: int, d_ff: int,\n",
        "                 dropout: float, pad_token_id: int):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.token_emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_token_id)\n",
        "        self.pos_enc = SinusoidalPositionalEncoding(d_model, max_len=max_len)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerEncoderLayer(d_model=d_model, n_heads=n_heads, d_ff=d_ff, dropout=dropout)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor):\n",
        "        x = self.token_emb(input_ids) * math.sqrt(self.d_model)\n",
        "        x = self.pos_enc(x)\n",
        "        x = self.dropout(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, attention_mask)\n",
        "        cls = x[:, 0, :]\n",
        "        return x, cls\n",
        "\n",
        "class TraceTransformerClassifier(nn.Module):\n",
        "    def __init__(self, encoder: TraceTransformerEncoder, d_model: int):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.head = nn.Linear(d_model, 1)\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor):\n",
        "        _, cls = self.encoder(input_ids, attention_mask)\n",
        "        logits = self.head(cls).squeeze(-1)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "b52cb3f0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train and Score utilities\n",
        "\n",
        "def train_one_epoch(model: nn.Module, loader: DataLoader, optimizer, device: torch.device) -> float:\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    n = 0\n",
        "    for batch in loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(input_ids, attention_mask)\n",
        "        loss = F.binary_cross_entropy_with_logits(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        bs = input_ids.size(0)\n",
        "        total_loss += loss.item() * bs\n",
        "        n += bs\n",
        "    return total_loss / max(1, n)\n",
        "\n",
        "def train_model(model: nn.Module, loader: DataLoader, device: torch.device,\n",
        "                lr: float, weight_decay: float, epochs: int) -> nn.Module:\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    for ep in range(1, epochs + 1):\n",
        "        loss = train_one_epoch(model, loader, optimizer, device)\n",
        "        if ep in {1, 2, 5, 10, epochs}:\n",
        "            print(f\"epoch {ep}/{epochs} - loss: {loss:.4f}\")\n",
        "    return model\n",
        "\n",
        "@torch.no_grad()\n",
        "def score_mean_prob(model: nn.Module, token_traces: List[List[int]], device: torch.device,\n",
        "                    batch_size: int = 256) -> float:\n",
        "    model.eval()\n",
        "    # Use a DataLoader with dynamic padding !!! dynamic for each batch\n",
        "    tmp_ds = [(t, 1.0) for t in token_traces]  \n",
        "    tmp_loader = DataLoader(tmp_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_token_traces)\n",
        "\n",
        "    probs = []\n",
        "    for batch in tmp_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        logits = model(input_ids, attention_mask)\n",
        "        probs.append(torch.sigmoid(logits).detach().cpu())\n",
        "    return torch.cat(probs).mean().item()\n",
        "\n",
        "def make_fresh_model() -> TraceTransformerClassifier:\n",
        "    enc = TraceTransformerEncoder(\n",
        "        vocab_size=len(stoi),\n",
        "        max_len=POSENC_MAXLEN,   # positional encoding length \n",
        "        d_model=D_MODEL,\n",
        "        n_layers=N_LAYERS,\n",
        "        n_heads=N_HEADS,\n",
        "        d_ff=D_FF,\n",
        "        dropout=DROPOUT,\n",
        "        pad_token_id=PAD_ID,\n",
        "    ).to(device)\n",
        "    return TraceTransformerClassifier(enc, d_model=D_MODEL).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9b69821",
      "metadata": {},
      "source": [
        "## Experiment 1\n",
        "For precision: train once\n",
        "For fitness train per model\n",
        "\n",
        "We have 15 models. \n",
        "We use epoch 50 based on scores we have tested.\n",
        "We generate 3000 traces in this experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ce38e6b",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Train Precision model once (event vs event_antilog)\n",
        "precision_model = make_fresh_model()\n",
        "prec_ds = TokenTraceBinaryDataset(event_token_traces, event_antilog_tokens, seed=SEED+1000)\n",
        "prec_loader = DataLoader(prec_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_token_traces)\n",
        "\n",
        "precision_model = train_model(precision_model, prec_loader, device, lr=LR, weight_decay=WEIGHT_DECAY, epochs=50)\n",
        "\n",
        "# Anchors (optional diagnostics)\n",
        "event_mean_prec = score_mean_prob(precision_model, event_token_traces, device)\n",
        "anti_mean_prec = score_mean_prob(precision_model, event_antilog_tokens, device)\n",
        "print(\"Precision model anchors -> event_mean:\", event_mean_prec, \"anti_mean:\", anti_mean_prec, \"gap:\", event_mean_prec - anti_mean_prec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# DEBUG CELL!!!\n",
        "# =========================\n",
        "\n",
        "EPOCHS_DEBUG = 55  # longer training only for debugging\n",
        "\n",
        "debug_models = [\"Model1\", \"Model2\", \"Model3\"]\n",
        "debug_rows = []\n",
        "\n",
        "for model_name in debug_models:\n",
        "    print(\"\\n[DEBUG] Training fitness model (longer) for\", model_name)\n",
        "    fit_model = make_fresh_model()\n",
        "\n",
        "    pos = model_token_traces_by_model[model_name]\n",
        "    neg = model_antilog_tokens_by_model[model_name]\n",
        "\n",
        "    fit_ds = TokenTraceBinaryDataset(pos, neg, seed=SEED + 9000 + (hash(model_name) % 10000))\n",
        "    fit_loader = DataLoader(fit_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_token_traces)\n",
        "\n",
        "    fit_model = train_model(fit_model, fit_loader, device, lr=LR, weight_decay=WEIGHT_DECAY, epochs=EPOCHS_DEBUG)\n",
        "\n",
        "    fitness_raw = score_mean_prob(fit_model, event_token_traces, device)\n",
        "    precision_raw = score_mean_prob(precision_model, pos, device)\n",
        "\n",
        "    debug_rows.append({\n",
        "        \"model\": model_name,\n",
        "        \"fitness_raw_debug\": fitness_raw,\n",
        "        \"precision_raw_debug\": precision_raw,\n",
        "    })\n",
        "\n",
        "pd.DataFrame(debug_rows)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea9be456",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Train Fitness models per model and compute scores\n",
        "\n",
        "results = []\n",
        "\n",
        "for model_name in sorted(model_token_traces_by_model.keys(), key=lambda s: int(s.replace(\"Model\",\"\"))):\n",
        "    fit_model = make_fresh_model()\n",
        "\n",
        "    pos = model_token_traces_by_model[model_name]\n",
        "    neg = model_antilog_tokens_by_model[model_name]\n",
        "\n",
        "    fit_ds = TokenTraceBinaryDataset(pos, neg, seed=SEED + 2000 + (hash(model_name) % 10000))\n",
        "    fit_loader = DataLoader(fit_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_token_traces)\n",
        "\n",
        "    print(\"\\nTraining fitness model for\", model_name)\n",
        "    fit_model = train_model(fit_model, fit_loader, device, lr=LR, weight_decay=WEIGHT_DECAY, epochs=50)\n",
        "\n",
        "    # Fitness: score event log with model-trained classifier\n",
        "    fitness_raw = score_mean_prob(fit_model, event_token_traces, device)\n",
        "\n",
        "    # Precision: score model log with event-trained classifier\n",
        "    precision_raw = score_mean_prob(precision_model, pos, device)\n",
        "\n",
        "    # Optional: per-model anchors (helps interpret raw scores)\n",
        "    model_mean = score_mean_prob(fit_model, pos, device)\n",
        "    model_anti_mean = score_mean_prob(fit_model, neg, device)\n",
        "    gap_model = model_mean - model_anti_mean\n",
        "\n",
        "    # Optional normalized fitness (anchors: model_mean=1, model_anti_mean=0)\n",
        "    # This is not for paper but for comparisons\n",
        "    if gap_model > 1e-6:\n",
        "        fitness_norm = float(max(0.0, min(1.0, (fitness_raw - model_anti_mean) / gap_model)))\n",
        "    else:\n",
        "        fitness_norm = float(\"nan\")\n",
        "\n",
        "    results.append({\n",
        "        \"model\": model_name,\n",
        "        \"fitness_raw\": fitness_raw,\n",
        "        \"fitness_norm\": fitness_norm,\n",
        "        \"precision_raw\": precision_raw,\n",
        "        \"fit_model_mean\": model_mean,\n",
        "        \"fit_anti_mean\": model_anti_mean,\n",
        "        \"fit_gap\": gap_model,\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "# Rounded display (3 decimals)\n",
        "df_rounded = df.copy()\n",
        "score_cols = [\n",
        "    \"fitness_raw\",\n",
        "    \"fitness_norm\",\n",
        "    \"precision_raw\",\n",
        "    \"fit_model_mean\",\n",
        "    \"fit_anti_mean\",\n",
        "    \"fit_gap\",\n",
        "]\n",
        "df_rounded[score_cols] = df_rounded[score_cols].round(3)\n",
        "\n",
        "df_rounded\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e63bb915",
      "metadata": {},
      "source": [
        "## Experiment 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a1457c1",
      "metadata": {},
      "source": [
        "In this experiment:\n",
        "4000 traces are generated for log/antilog pairs.\n",
        "Trained on 22 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81b458ed",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import math\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# PM4Py\n",
        "from pm4py.objects.petri_net.importer import importer as pnml_importer\n",
        "from pm4py.objects.process_tree.importer import importer as ptml_importer\n",
        "from pm4py.objects.conversion.process_tree import converter as pt_converter\n",
        "from pm4py.algo.simulation.playout.petri_net import algorithm as pn_playout\n",
        "\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "USE_AMP = False \n",
        "\n",
        "print(\"Device:\", DEVICE, \"| AMP:\", USE_AMP)\n",
        "\n",
        "\n",
        "BASE = \"../Data/Experiment2\"\n",
        "CSV_DIR = os.path.join(BASE, \"CSV_Control_Flow\")\n",
        "PT_DIR  = os.path.join(BASE, \"ProcessTrees\")\n",
        "PN_DIR  = os.path.join(BASE, \"Petri_Nets\")\n",
        "\n",
        "assert os.path.isdir(CSV_DIR), f\"Missing CSV dir: {CSV_DIR}\"\n",
        "\n",
        "def load_csv_trace_table(csv_path: str,\n",
        "                         bos_token: str = \"BOS\",\n",
        "                         eos_token: str = \"EOS\") -> List[List[str]]:\n",
        "    df = pd.read_csv(csv_path, header=0)\n",
        "    if len(df.columns) > 0 and str(df.columns[0]).lower().startswith(\"unnamed\"):\n",
        "        df = df.drop(columns=[df.columns[0]])\n",
        "\n",
        "    traces: List[List[str]] = []\n",
        "    for _, row in df.iterrows():\n",
        "        toks: List[str] = []\n",
        "        for cell in row.values:\n",
        "            if pd.isna(cell):\n",
        "                continue\n",
        "            t = str(cell).strip()\n",
        "            if t == \"\" or t.lower() == \"nan\":\n",
        "                continue\n",
        "            toks.append(t)\n",
        "\n",
        "        if not toks:\n",
        "            continue\n",
        "        toks = [t for t in toks if t not in (bos_token, eos_token)]\n",
        "        if toks:\n",
        "            traces.append(toks)\n",
        "\n",
        "    return traces\n",
        "\n",
        "def model_tag(choice:int, parallel:int, loop:int) -> str:\n",
        "    return f\"Choice{choice:02d}_Parallel{parallel:02d}_Loop{loop:02d}\"\n",
        "\n",
        "def ptml_path(tag: str) -> str:\n",
        "    return os.path.join(PT_DIR, f\"PT_{tag}.ptml\")\n",
        "\n",
        "def pnml_path(tag: str) -> str:\n",
        "    return os.path.join(PN_DIR, f\"PN_{tag}.pnml\")\n",
        "\n",
        "def load_model_as_petri(tag: str):\n",
        "    ptml = ptml_path(tag)\n",
        "    pnml = pnml_path(tag)\n",
        "    if os.path.isfile(ptml):\n",
        "        pt = ptml_importer.apply(ptml)\n",
        "        net, im, fm = pt_converter.apply(pt, variant=pt_converter.Variants.TO_PETRI_NET)\n",
        "        return net, im, fm\n",
        "    if os.path.isfile(pnml):\n",
        "        net, im, fm = pnml_importer.apply(pnml)\n",
        "        return net, im, fm\n",
        "    raise FileNotFoundError(f\"No PTML/PNML found for {tag}: {ptml} / {pnml}\")\n",
        "\n",
        "def playout_to_traces(net, im, fm, n_traces: int = 4000, max_trace_length: int = 200) -> List[List[str]]:\n",
        "    # PM4Py playout returns an event log.\n",
        "    params = {\n",
        "        \"no_traces\": n_traces,\n",
        "        \"max_trace_length\": max_trace_length,\n",
        "        \"activity_key\": \"concept:name\",\n",
        "    }\n",
        "    elog = pn_playout.apply(net, im, fm, variant=pn_playout.Variants.BASIC_PLAYOUT, parameters=params)\n",
        "\n",
        "    traces: List[List[str]] = []\n",
        "    for case in elog:\n",
        "        tr = []\n",
        "        for ev in case:\n",
        "            if \"concept:name\" in ev:\n",
        "                tr.append(ev[\"concept:name\"])\n",
        "        if tr:\n",
        "            traces.append(tr)\n",
        "    return traces\n",
        "\n",
        "PAD = \"<PAD>\"\n",
        "UNK = \"<UNK>\"\n",
        "CLS = \"<CLS>\"\n",
        "\n",
        "def build_vocab(traces: List[List[str]], extra_tokens: Optional[List[str]] = None) -> Dict[str,int]:\n",
        "    vocab = {PAD:0, UNK:1, CLS:2}\n",
        "    if extra_tokens:\n",
        "        for t in extra_tokens:\n",
        "            if t not in vocab:\n",
        "                vocab[t] = len(vocab)\n",
        "    for tr in traces:\n",
        "        for a in tr:\n",
        "            if a not in vocab:\n",
        "                vocab[a] = len(vocab)\n",
        "    return vocab\n",
        "\n",
        "def tokenize_trace(trace: List[str], vocab: Dict[str,int], max_len: int) -> List[int]:\n",
        "    ids = [vocab.get(CLS, 2)]\n",
        "    for a in trace[: max_len-1]:\n",
        "        ids.append(vocab.get(a, vocab[UNK]))\n",
        "    return ids\n",
        "\n",
        "def corrupt_trace(trace: List[str], alphabet: List[str], noise: str, level: float) -> List[str]:\n",
        "    \"\"\"Apply one noise operator to a trace.\n",
        "    level in [0,1] is probability/ratio controlling number of edits.\n",
        "    \"\"\"\n",
        "    if not trace:\n",
        "        return trace\n",
        "    tr = trace.copy()\n",
        "    n_edits = max(1, int(round(level * len(tr))))\n",
        "    n_edits = min(n_edits, max(1, len(tr)))\n",
        "\n",
        "    rng = random.Random(SEED + len(trace) + n_edits)\n",
        "\n",
        "    if noise == \"delete\":\n",
        "        for _ in range(n_edits):\n",
        "            if not tr:\n",
        "                break\n",
        "            idx = rng.randrange(len(tr))\n",
        "            tr.pop(idx)\n",
        "        return tr if tr else trace[:1]\n",
        "\n",
        "    if noise == \"add\":\n",
        "        for _ in range(n_edits):\n",
        "            idx = rng.randrange(len(tr)+1)\n",
        "            tr.insert(idx, rng.choice(alphabet))\n",
        "        return tr\n",
        "\n",
        "    if noise == \"replace\":\n",
        "        for _ in range(n_edits):\n",
        "            idx = rng.randrange(len(tr))\n",
        "            tr[idx] = rng.choice(alphabet)\n",
        "        return tr\n",
        "\n",
        "    if noise == \"swap\":\n",
        "        for _ in range(n_edits):\n",
        "            if len(tr) < 2:\n",
        "                break\n",
        "            i = rng.randrange(len(tr)-1)\n",
        "            tr[i], tr[i+1] = tr[i+1], tr[i]\n",
        "        return tr\n",
        "\n",
        "    return tr\n",
        "\n",
        "def generate_antilog_hybrid(positives: List[List[str]],\n",
        "                            alphabet: List[str],\n",
        "                            size: int,\n",
        "                            noise_types: List[str] = (\"add\",\"delete\",\"replace\",\"swap\"),\n",
        "                            level: float = 0.3) -> List[List[str]]:\n",
        "    \"\"\"Generate negatives by corrupting randomly sampled positive traces.\"\"\"\n",
        "    negs: List[List[str]] = []\n",
        "    rng = random.Random(SEED + 999)\n",
        "    while len(negs) < size:\n",
        "        base = rng.choice(positives)\n",
        "        op = rng.choice(list(noise_types))\n",
        "        neg = corrupt_trace(base, alphabet, op, level)\n",
        "        negs.append(neg)\n",
        "    return negs\n",
        "\n",
        "\n",
        "class SeqDataset(Dataset):\n",
        "    def __init__(self, X: List[List[int]], y: List[int]):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "def collate_dynamic_pad(batch):\n",
        "    xs, ys = zip(*batch)\n",
        "    maxlen = max(len(x) for x in xs)\n",
        "    x_pad = torch.zeros(len(xs), maxlen, dtype=torch.long)\n",
        "    attn  = torch.zeros(len(xs), maxlen, dtype=torch.bool)\n",
        "    for i,x in enumerate(xs):\n",
        "        x_pad[i, :len(x)] = torch.tensor(x, dtype=torch.long)\n",
        "        attn[i, :len(x)] = True\n",
        "    y = torch.tensor(ys, dtype=torch.long)\n",
        "    return x_pad, attn, y\n",
        "\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SinusoidalPositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Standard sinusoidal positional encoding (Vaswani et al.).\n",
        "    Adds fixed position-dependent vectors to token embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int, max_len: int = 512):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)  # (T, D)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # (T, 1)\n",
        "\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2, dtype=torch.float) * (-math.log(10000.0) / d_model)\n",
        "        )  # (D/2,)\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)  # even dims\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)  # odd dims\n",
        "\n",
        "        self.register_buffer(\"pe\", pe.unsqueeze(0))  # (1, T, D)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x: (B, T, D)\n",
        "        returns: (B, T, D) with positional encoding added\n",
        "        \"\"\"\n",
        "        T = x.size(1)\n",
        "        return x + self.pe[:, :T, :]\n",
        "\n",
        "\n",
        "class TransformerClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder-only Transformer discriminator.\n",
        "    Input: token ids with [CLS] at position 0, padded with PAD=0.\n",
        "    Uses sinusoidal positional encoding + attention mask to ignore PAD.\n",
        "    Output: logits for 2 classes (log vs antilog), matching your existing BCE/CE setup.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        max_len: int,\n",
        "        d_model: int = 128,\n",
        "        n_heads: int = 4,\n",
        "        n_layers: int = 2,\n",
        "        d_ff: int = 512,\n",
        "        dropout: float = 0.0,\n",
        "        pad_idx: int = 0,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
        "        self.pos = SinusoidalPositionalEncoding(d_model=d_model, max_len=max_len)\n",
        "\n",
        "        enc_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=d_ff,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "            activation=\"gelu\",\n",
        "            norm_first=False,\n",
        "        )\n",
        "        self.enc = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # 2-class head (log vs antilog). If you use a 1-logit sigmoid, change this to nn.Linear(d_model, 1).\n",
        "        self.fc = nn.Linear(d_model, 2)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, attn_mask_bool: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x: (B, T) token ids\n",
        "        attn_mask_bool: (B, T) True for real tokens, False for PAD\n",
        "        returns: logits (B, 2)\n",
        "        \"\"\"\n",
        "        # Embedding + positional encoding\n",
        "        h = self.emb(x)           # (B, T, D)\n",
        "        h = self.pos(h)           # (B, T, D)\n",
        "        h = self.dropout(h)\n",
        "\n",
        "        src_key_padding_mask = ~attn_mask_bool  # (B, T)\n",
        "\n",
        "        h = self.enc(h, src_key_padding_mask=src_key_padding_mask)  # (B, T, D)\n",
        "\n",
        "        # CLS pooling: take position 0\n",
        "        cls = h[:, 0, :]  # (B, D)\n",
        "        logits = self.fc(cls)\n",
        "        return logits\n",
        "\n",
        "@dataclass\n",
        "class TrainCfg:\n",
        "    lr: float = 2e-4\n",
        "    epochs: int = 22\n",
        "    batch_size: int = 32\n",
        "    max_len: int = 96\n",
        "    d_model: int = 128\n",
        "    n_heads: int = 4\n",
        "    n_layers: int = 2\n",
        "    dropout: float = 0.1\n",
        "\n",
        "CFG = TrainCfg()\n",
        "\n",
        "def train_discriminator(model: nn.Module, dl: DataLoader, epochs: int, lr: float, use_amp: bool=False) -> None:\n",
        "    model.train()\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "        total_loss = 0.0\n",
        "        total = 0\n",
        "        for x, attn, y in dl:\n",
        "            x = x.to(DEVICE)\n",
        "            attn = attn.to(DEVICE)\n",
        "            y = y.to(DEVICE)\n",
        "\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            logits = model(x, attn)\n",
        "            loss = F.cross_entropy(logits, y)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            total_loss += float(loss.item()) * y.size(0)\n",
        "            total += y.size(0)\n",
        "\n",
        "        print(f\"  epoch {ep:02d}/{epochs} | loss={total_loss/max(1,total):.4f}\")\n",
        "\n",
        "@torch.no_grad()\n",
        "def score_mean_positive_prob(model: nn.Module, X_tok: List[List[int]]) -> float:\n",
        "    \"\"\"Return mean P(class=1) over X_tok.\"\"\"\n",
        "    model.eval()\n",
        "    bs = 256\n",
        "    probs = []\n",
        "    for i in range(0, len(X_tok), bs):\n",
        "        chunk = X_tok[i:i+bs]\n",
        "        maxlen = max(len(x) for x in chunk)\n",
        "        x_pad = torch.zeros(len(chunk), maxlen, dtype=torch.long)\n",
        "        attn  = torch.zeros(len(chunk), maxlen, dtype=torch.bool)\n",
        "        for j,x in enumerate(chunk):\n",
        "            x_pad[j,:len(x)] = torch.tensor(x, dtype=torch.long)\n",
        "            attn[j,:len(x)] = True\n",
        "        x_pad = x_pad.to(DEVICE)\n",
        "        attn  = attn.to(DEVICE)\n",
        "        logits = model(x_pad, attn)\n",
        "        p = F.softmax(logits, dim=-1)[:,1]\n",
        "        probs.append(p.detach().cpu().numpy())\n",
        "    return float(np.mean(np.concatenate(probs))) if probs else float(\"nan\")\n",
        "\n",
        "TAG_RE = re.compile(r\"Choice\\d{2}_Parallel\\d{2}_Loop\\d{2}\")\n",
        "FILE_RE = re.compile(r\"Event_Log_(add|delete|replace|swap)_(\\d+)\\.csv$\", re.I)\n",
        "\n",
        "def discover_event_logs(csv_root: str) -> Dict[str, Dict[Tuple[str,int], str]]:\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      dict[tag][(noise_type, level_int)] = csv_path\n",
        "    Also detects clean logs as (noise_type='clean', level=0) when filename doesn't match FILE_RE.\n",
        "    \"\"\"\n",
        "    mapping: Dict[str, Dict[Tuple[str,int], str]] = {}\n",
        "\n",
        "    for root, _, files in os.walk(csv_root):\n",
        "        for fn in files:\n",
        "            if not fn.lower().endswith(\".csv\"):\n",
        "                continue\n",
        "            full = os.path.join(root, fn)\n",
        "\n",
        "            # Determine tag from path components\n",
        "            parts = full.replace(\"\\\\\", \"/\").split(\"/\")\n",
        "            tag = None\n",
        "            for p in reversed(parts):\n",
        "                if TAG_RE.fullmatch(p):\n",
        "                    tag = p\n",
        "                    break\n",
        "            if tag is None:\n",
        "                mtag = TAG_RE.search(fn)\n",
        "                if mtag:\n",
        "                    tag = mtag.group(0)\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "            mapping.setdefault(tag, {})\n",
        "\n",
        "            m = FILE_RE.search(fn)\n",
        "            if m:\n",
        "                ntype = m.group(1).lower()\n",
        "                lvl = int(m.group(2))\n",
        "                mapping[tag][(ntype, lvl)] = full\n",
        "            else:\n",
        "                # treat as clean\n",
        "                mapping[tag][(\"clean\", 0)] = full\n",
        "\n",
        "    return mapping\n",
        "\n",
        "event_logs = discover_event_logs(CSV_DIR)\n",
        "\n",
        "if not event_logs:\n",
        "    raise RuntimeError(f\"No CSV logs discovered under {CSV_DIR}. Check naming like Event_Log_add_10.csv and folder tags Choice..\")\n",
        "\n",
        "all_tags = sorted(event_logs.keys())\n",
        "print(\"Discovered structures:\", len(all_tags))\n",
        "print(\"Example tags:\", all_tags[:5])\n",
        "\n",
        "# Determine available levels per noise type (from data!)\n",
        "noise_types = [\"add\",\"delete\",\"replace\",\"swap\"]\n",
        "levels_by_noise: Dict[str, List[int]] = {nt: [] for nt in noise_types}\n",
        "for tag in all_tags:\n",
        "    for (nt, lvl), _ in event_logs[tag].items():\n",
        "        if nt in noise_types:\n",
        "            levels_by_noise[nt].append(lvl)\n",
        "\n",
        "for nt in noise_types:\n",
        "    lvls = sorted(set(levels_by_noise[nt]))\n",
        "    levels_by_noise[nt] = lvls\n",
        "    print(f\"Noise '{nt}': levels found:\", lvls)\n",
        "\n",
        "results = []\n",
        "\n",
        "for tag in all_tags:\n",
        "    print(\"\\n==============================\")\n",
        "    print(\"Structure:\", tag)\n",
        "\n",
        "    # Load model log once\n",
        "    net, im, fm = load_model_as_petri(tag)\n",
        "    model_traces = playout_to_traces(net, im, fm, n_traces=4000, max_trace_length=200)\n",
        "\n",
        "    # Build alphabet and vocab\n",
        "    alphabet = sorted({a for tr in model_traces for a in tr})\n",
        "    vocab = build_vocab(model_traces)\n",
        "\n",
        "    # Tokenize model positives\n",
        "    X_pos = [tokenize_trace(tr, vocab, CFG.max_len) for tr in model_traces]\n",
        "\n",
        "    # Create antilog negatives (size matched to positives)\n",
        "    neg_traces = generate_antilog_hybrid(model_traces, alphabet, size=len(model_traces), level=0.3)\n",
        "    X_neg = [tokenize_trace(tr, vocab, CFG.max_len) for tr in neg_traces]\n",
        "\n",
        "    # Train discriminator once per structure\n",
        "    X = X_pos + X_neg\n",
        "    y = [1]*len(X_pos) + [0]*len(X_neg)\n",
        "\n",
        "    ds = SeqDataset(X, y)\n",
        "    dl = DataLoader(ds, batch_size=CFG.batch_size, shuffle=True, collate_fn=collate_dynamic_pad, num_workers=0)\n",
        "    vocab_size = len(vocab)\n",
        "    clf  = TransformerClassifier(\n",
        "     vocab_size=vocab_size,\n",
        "     max_len=CFG.max_len,\n",
        "     d_model=128,\n",
        "     n_heads=4,\n",
        "     n_layers=2,\n",
        "     d_ff=512,\n",
        "     dropout=0.0,\n",
        "     pad_idx=0\n",
        " ).to(DEVICE)\n",
        "\n",
        "    train_discriminator(clf, dl, epochs=CFG.epochs, lr=CFG.lr, use_amp=USE_AMP)\n",
        "\n",
        "    \n",
        "    # Optionally score clean (no-noise) event log for this structure if present\n",
        "    clean_path = event_logs.get(tag, {}).get((\"clean\", 0))\n",
        "    if clean_path:\n",
        "        ev_clean = load_csv_trace_table(clean_path)\n",
        "        X_clean = [tokenize_trace(tr, vocab, CFG.max_len) for tr in ev_clean]\n",
        "        clean_score = score_mean_positive_prob(clf, X_clean)\n",
        "        results.append({\n",
        "            \"structure\": tag,\n",
        "            \"noise_type\": \"clean\",\n",
        "            \"noise_level\": 0,\n",
        "            \"fitness_score\": float(clean_score),\n",
        "            \"n_event_traces\": len(ev_clean),\n",
        "            \"n_model_traces\": len(model_traces),\n",
        "            \"model_source\": \"PTML\" if os.path.isfile(ptml_path(tag)) else \"PNML\",\n",
        "        })\n",
        "# Score each available noisy CSV for this structure\n",
        "    for nt in noise_types:\n",
        "        for lvl in levels_by_noise[nt]:\n",
        "            csv_path = event_logs.get(tag, {}).get((nt, lvl))\n",
        "            if not csv_path:\n",
        "                continue\n",
        "            ev_traces = load_csv_trace_table(csv_path)\n",
        "            X_ev = [tokenize_trace(tr, vocab, CFG.max_len) for tr in ev_traces]\n",
        "            score = score_mean_positive_prob(clf, X_ev)\n",
        "\n",
        "            results.append({\n",
        "                \"structure\": tag,\n",
        "                \"noise_type\": nt,\n",
        "                \"noise_level\": int(lvl),\n",
        "                \"fitness_score\": float(score),\n",
        "                \"n_event_traces\": len(ev_traces),\n",
        "                \"n_model_traces\": len(model_traces),\n",
        "                \"model_source\": \"PTML\" if os.path.isfile(ptml_path(tag)) else \"PNML\",\n",
        "            })\n",
        "\n",
        "df_results = pd.DataFrame(results)\n",
        "print(\"\\nDone. Rows:\", len(df_results))\n",
        "display(df_results.head())\n",
        "\n",
        "\n",
        "# Save raw results\n",
        "os.makedirs(BASE, exist_ok=True)\n",
        "out_csv = os.path.join(BASE, \"experiment2_transformer_results.csv\")\n",
        "df_results.to_csv(out_csv, index=False)\n",
        "print(\"Saved:\", out_csv)\n",
        "\n",
        "agg_rows = []\n",
        "for nt in noise_types:\n",
        "    for lvl in sorted(df_results.loc[df_results[\"noise_type\"]==nt, \"noise_level\"].unique()):\n",
        "        sub = df_results[(df_results[\"noise_type\"]==nt) & (df_results[\"noise_level\"]==lvl)]\n",
        "        if sub.empty:\n",
        "            continue\n",
        "        agg_rows.append({\n",
        "            \"noise_type\": nt,\n",
        "            \"noise_level\": int(lvl),\n",
        "            \"mean\": float(sub[\"fitness_score\"].mean()),\n",
        "            \"min\": float(sub[\"fitness_score\"].min()),\n",
        "            \"max\": float(sub[\"fitness_score\"].max()),\n",
        "            \"n_structures\": int(sub[\"structure\"].nunique()),\n",
        "        })\n",
        "df_agg = pd.DataFrame(agg_rows).sort_values([\"noise_type\",\"noise_level\"])\n",
        "clean_sub = df_results[df_results[\"noise_type\"]==\"clean\"]\n",
        "if not clean_sub.empty:\n",
        "    base = {\n",
        "        \"mean\": float(clean_sub[\"fitness_score\"].mean()),\n",
        "        \"min\": float(clean_sub[\"fitness_score\"].min()),\n",
        "        \"max\": float(clean_sub[\"fitness_score\"].max()),\n",
        "        \"n_structures\": int(clean_sub[\"structure\"].nunique()),\n",
        "    }\n",
        "    baseline_rows = []\n",
        "    for nt in noise_types:\n",
        "        baseline_rows.append({\n",
        "            \"noise_type\": nt,\n",
        "            \"noise_level\": 0,\n",
        "            **base\n",
        "        })\n",
        "    df_agg = pd.concat([pd.DataFrame(baseline_rows), df_agg], ignore_index=True).sort_values([\"noise_type\",\"noise_level\"])\n",
        "\n",
        "display(df_agg)\n",
        "\n",
        "# Plot (2Ã—2 grid): add/delete/replace/swap\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i, nt in enumerate(noise_types):\n",
        "    ax = axes[i]\n",
        "    sub = df_agg[df_agg[\"noise_type\"]==nt].sort_values(\"noise_level\")\n",
        "    x = sub[\"noise_level\"].to_numpy()\n",
        "    y = sub[\"mean\"].to_numpy()\n",
        "    yerr_low = y - sub[\"min\"].to_numpy()\n",
        "    yerr_high = sub[\"max\"].to_numpy() - y\n",
        "    ax.errorbar(x, y, yerr=[yerr_low, yerr_high], marker=\"o\", capsize=3)\n",
        "    ax.set_title(nt)\n",
        "    ax.set_xlabel(\"Noise level (%)\")\n",
        "    ax.set_ylabel(\"Fitness score (mean Â± min/max across structures)\")\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "out_png = os.path.join(BASE, \"experiment2_fitness_plots.png\")\n",
        "plt.savefig(out_png, dpi=200)\n",
        "plt.show()\n",
        "print(\"Saved:\", out_png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66a2e37e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Results plot\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i, nt in enumerate(noise_types):\n",
        "    ax = axes[i]\n",
        "    sub = df_agg[df_agg[\"noise_type\"]==nt].sort_values(\"noise_level\")\n",
        "    x = sub[\"noise_level\"].to_numpy()\n",
        "    y = sub[\"mean\"].to_numpy()\n",
        "    yerr_low = y - sub[\"min\"].to_numpy()\n",
        "    yerr_high = sub[\"max\"].to_numpy() - y\n",
        "    ax.errorbar(x, y, yerr=[yerr_low, yerr_high], marker=\"o\", capsize=3)\n",
        "    ax.set_title(nt)\n",
        "    ax.set_xlabel(\"Noise level (%)\")\n",
        "    ax.set_ylabel(\"Fitness score (mean Â± min/max across structures)\")\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save PNG\n",
        "out_png = os.path.join(BASE, \"experiment2_fitness_plots.png\")\n",
        "plt.savefig(out_png, dpi=200)\n",
        "\n",
        "# Save PDF (vector, best for papers)\n",
        "out_pdf = os.path.join(BASE, \"experiment2_fitness_plots.pdf\")\n",
        "plt.savefig(out_pdf, bbox_inches=\"tight\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"Saved:\", out_png)\n",
        "print(\"Saved:\", out_pdf)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3566980",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot Experiment 2: Our Transformer vs Reference Work\n",
        "\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "BASE = \"../Data/Experiment2\"\n",
        "OUR_CSV = os.path.join(BASE, \"experiment2_transformer_results.csv\")\n",
        "\n",
        "REF_DIR = \"../Data/Full_Results_Experiment2\" \n",
        "\n",
        "REF_FILES = {\n",
        "    \"Ref: LSTM\": \"LSTM_Results_Experiment2.csv\",\n",
        "    \"Ref: Unigram WMD\": \"Unigram_WMD_Results_Experiment2.csv\",\n",
        "    \"Ref: Unigram ICT\": \"Unigram_ICT_Results_Experiment2.csv\",\n",
        "    \"Ref: Unigram t2v\": \"Unigram_t2v_Results_Experiment2.csv\",\n",
        "    \"Ref: Bigram WMD\": \"Bigram_WMD_Results_Experiment2.csv\",\n",
        "    \"Ref: Bigram ICT\": \"Bigram_ICT_Results_Experiment2.csv\",\n",
        "    \"Ref: Bigram t2v\": \"Bigram_t2v_Results_Experiment2.csv\",\n",
        "}\n",
        "\n",
        "noise_types = [\"add\", \"delete\", \"replace\", \"swap\"]\n",
        "\n",
        "def agg_ours(df_ours: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    df_ours columns expected:\n",
        "      structure, noise_type, noise_level, fitness_score\n",
        "    Returns aggregated rows per (noise_type, noise_level): mean/min/max\n",
        "    \"\"\"\n",
        "    df = df_ours.copy()\n",
        "    df[\"noise_type\"] = df[\"noise_type\"].str.lower()\n",
        "\n",
        "    out = (\n",
        "        df[df[\"noise_type\"].isin(noise_types + [\"clean\"])]\n",
        "        .groupby([\"noise_type\", \"noise_level\"], as_index=False)[\"fitness_score\"]\n",
        "        .agg(mean=\"mean\", min=\"min\", max=\"max\")\n",
        "        .sort_values([\"noise_type\", \"noise_level\"])\n",
        "    )\n",
        "    return out\n",
        "\n",
        "def load_ref_csv(path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Reference CSV format (typical):\n",
        "      Choice, Parallel, Loop, Noise Level, Add, Delete, Replace, Swap\n",
        "    Noise Level can be like '5%' or '5' or 5\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(path)\n",
        "\n",
        "    # normalize column names\n",
        "    df.columns = [c.strip() for c in df.columns]\n",
        "\n",
        "    # detect noise-level col name\n",
        "    nl_col = None\n",
        "    for cand in [\"Noise Level\", \"Noise level\", \"Noise\", \"NoiseLevel\", \"noise_level\"]:\n",
        "        if cand in df.columns:\n",
        "            nl_col = cand\n",
        "            break\n",
        "    if nl_col is None:\n",
        "        raise ValueError(f\"Could not find a Noise Level column in {os.path.basename(path)}. Columns: {df.columns.tolist()}\")\n",
        "\n",
        "    def parse_lvl(x):\n",
        "        if pd.isna(x):\n",
        "            return np.nan\n",
        "        s = str(x).strip()\n",
        "        s = s.replace(\"%\", \"\")\n",
        "        return int(float(s))\n",
        "    df[\"noise_level\"] = df[nl_col].apply(parse_lvl)\n",
        "\n",
        "    col_map = {}\n",
        "    for nt in noise_types:\n",
        "        for c in df.columns:\n",
        "            if c.strip().lower() == nt:\n",
        "                col_map[nt] = c\n",
        "                break\n",
        "    missing = [nt for nt in noise_types if nt not in col_map]\n",
        "    if missing:\n",
        "        raise ValueError(f\"Missing noise columns {missing} in {os.path.basename(path)}. Columns: {df.columns.tolist()}\")\n",
        "\n",
        "    long = []\n",
        "    for nt in noise_types:\n",
        "        tmp = df[[\"noise_level\", col_map[nt]]].copy()\n",
        "        tmp.rename(columns={col_map[nt]: \"fitness_score\"}, inplace=True)\n",
        "        tmp[\"noise_type\"] = nt\n",
        "        long.append(tmp)\n",
        "\n",
        "    out = pd.concat(long, ignore_index=True)\n",
        "    out[\"fitness_score\"] = pd.to_numeric(out[\"fitness_score\"], errors=\"coerce\")\n",
        "    out = out.dropna(subset=[\"noise_level\", \"fitness_score\"])\n",
        "\n",
        "    # Aggregate across structures (each row is one structure configuration)\n",
        "    agg = (\n",
        "        out.groupby([\"noise_type\", \"noise_level\"], as_index=False)[\"fitness_score\"]\n",
        "           .agg(mean=\"mean\", min=\"min\", max=\"max\")\n",
        "           .sort_values([\"noise_type\", \"noise_level\"])\n",
        "    )\n",
        "    return agg\n",
        "\n",
        "assert os.path.isfile(OUR_CSV), f\"Missing our results CSV at: {OUR_CSV}\"\n",
        "df_ours_raw = pd.read_csv(OUR_CSV)\n",
        "df_ours_agg = agg_ours(df_ours_raw)\n",
        "\n",
        "\n",
        "ref_aggs = {}\n",
        "for label, fn in REF_FILES.items():\n",
        "    fpath = os.path.join(REF_DIR, fn)\n",
        "    if not os.path.isfile(fpath):\n",
        "        print(f\"[skip] Not found: {fpath}\")\n",
        "        continue\n",
        "    ref_aggs[label] = load_ref_csv(fpath)\n",
        "\n",
        "if not ref_aggs:\n",
        "    raise RuntimeError(\n",
        "        \"No reference CSVs were loaded. Check REF_DIR and filenames in REF_FILES.\"\n",
        "    )\n",
        "\n",
        "# Plot\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i, nt in enumerate(noise_types):\n",
        "    ax = axes[i]\n",
        "\n",
        "    sub_o = df_ours_agg[df_ours_agg[\"noise_type\"] == nt].sort_values(\"noise_level\")\n",
        "    if not sub_o.empty:\n",
        "        x = sub_o[\"noise_level\"].to_numpy()\n",
        "        y = sub_o[\"mean\"].to_numpy()\n",
        "        ax.plot(x, y, marker=\"o\", label=\"Ours: Transformer (mean)\")\n",
        "        yerr_low = y - sub_o[\"min\"].to_numpy()\n",
        "        yerr_high = sub_o[\"max\"].to_numpy() - y\n",
        "        ax.errorbar(x, y, yerr=[yerr_low, yerr_high], capsize=3, alpha=0.35)\n",
        "\n",
        "    for label, agg in ref_aggs.items():\n",
        "        sub_r = agg[agg[\"noise_type\"] == nt].sort_values(\"noise_level\")\n",
        "        if sub_r.empty:\n",
        "            continue\n",
        "        xr = sub_r[\"noise_level\"].to_numpy()\n",
        "        yr = sub_r[\"mean\"].to_numpy()\n",
        "        ax.plot(xr, yr, marker=\"o\", linestyle=\"--\", label=label)\n",
        "\n",
        "    ax.set_title(nt)\n",
        "    ax.set_xlabel(\"Noise level (%)\")\n",
        "    ax.set_ylabel(\"Fitness score\")\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.set_ylim(0.0, 1.02)\n",
        "\n",
        "# one legend for whole figure\n",
        "handles, labels = axes[0].get_legend_handles_labels()\n",
        "fig.legend(handles, labels, loc=\"lower center\", ncol=2, frameon=False)\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.06, 1, 1])\n",
        "\n",
        "out_png = os.path.join(BASE, \"experiment2_fitness_ours_vs_reference.png\")\n",
        "out_pdf = os.path.join(BASE, \"experiment2_fitness_ours_vs_reference.pdf\")\n",
        "plt.savefig(out_png, dpi=200)\n",
        "plt.savefig(out_pdf, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Saved:\", out_png)\n",
        "print(\"Saved:\", out_pdf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc97e42a",
      "metadata": {},
      "source": [
        "## Experiment 3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0c28ad7",
      "metadata": {},
      "source": [
        "Experiment 3 is conducted using 30k traces. It is not enough but due to computational limitations, this is the case.\n",
        "14 Epochs are used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0d9da5d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "# This is a debug cell to see if we have correct normalizations of activities or not. \n",
        "def norm_act(a: str) -> str:\n",
        "    a = a.strip().lower()\n",
        "    a = re.sub(r\"\\s+\", \"\", a)\n",
        "    a = re.sub(r\"[()]\", \"\", a)\n",
        "    a = re.sub(r\"[,&\\-]\", \"\", a)\n",
        "    return a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6342845",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pm4py.objects.petri_net.importer import importer as pnml_importer\n",
        "from pm4py.algo.simulation.playout.petri_net import algorithm as pn_playout\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "USE_AMP = False  # keep FP32 for ROCm stability\n",
        "print(\"Device:\", DEVICE, \"| AMP:\", USE_AMP)\n",
        "\n",
        "BASE = \"../Data/Experiment3\"\n",
        "MODELS_DIR = os.path.join(BASE, \"Models\")\n",
        "LOGS_DIR   = os.path.join(BASE, \"Only_Control_Flow_csv\")\n",
        "\n",
        "assert os.path.isdir(MODELS_DIR), f\"Missing: {MODELS_DIR}\"\n",
        "assert os.path.isdir(LOGS_DIR), f\"Missing: {LOGS_DIR}\"\n",
        "\n",
        "# DATASETS = [\"BPIC_2017\"]\n",
        "DATASETS = [\"BPIC_2017\", \"Helpdesk\", \"Hospital_filtered\", \"Hospital\", \"RTFM\"]\n",
        "\n",
        "\n",
        "def load_csv_trace_table(csv_path: str,\n",
        "                         bos_token: str = \"BOS\",\n",
        "                         eos_token: str = \"EOS\") -> List[List[str]]:\n",
        "    df = pd.read_csv(csv_path, header=0)\n",
        "    # drop accidental index column\n",
        "    if len(df.columns) > 0 and str(df.columns[0]).lower().startswith(\"unnamed\"):\n",
        "        df = df.drop(columns=[df.columns[0]])\n",
        "\n",
        "    traces: List[List[str]] = []\n",
        "    for _, row in df.iterrows():\n",
        "        toks: List[str] = []\n",
        "        for cell in row.values:\n",
        "            if pd.isna(cell):\n",
        "                continue\n",
        "            t = str(cell).strip()\n",
        "            if t == \"\" or t.lower() == \"nan\":\n",
        "                continue\n",
        "            toks.append(norm_act(t))\n",
        "\n",
        "        if not toks:\n",
        "            continue\n",
        "        toks = [t for t in toks if t not in (bos_token, eos_token)]\n",
        "        if toks:\n",
        "            traces.append(toks)\n",
        "    return traces\n",
        "\n",
        "# Model discovery: <DATASET>_<LEVEL>.pnml  (LEVEL = 00/25/50/75/100 etc.)\n",
        "MODEL_RE = re.compile(r\"^(?P<name>.+)_(?P<lvl>\\d{2,3})\\.pnml$\", re.I)\n",
        "\n",
        "def discover_models(models_dir: str) -> Dict[str, List[Tuple[int, str]]]:\n",
        "    \"\"\"\n",
        "    Returns mapping: dataset_name -> list of (level_int, file_path) sorted by level\n",
        "    \"\"\"\n",
        "    mapping: Dict[str, List[Tuple[int, str]]] = {}\n",
        "    for fn in os.listdir(models_dir):\n",
        "        m = MODEL_RE.match(fn)\n",
        "        if not m:\n",
        "            continue\n",
        "        name = m.group(\"name\")\n",
        "        lvl = int(m.group(\"lvl\"))\n",
        "        path = os.path.join(models_dir, fn)\n",
        "        mapping.setdefault(name, []).append((lvl, path))\n",
        "    for name in mapping:\n",
        "        mapping[name] = sorted(mapping[name], key=lambda x: x[0])\n",
        "    return mapping\n",
        "\n",
        "models_map = discover_models(MODELS_DIR)\n",
        "\n",
        "def load_pnml_model(pnml_path: str):\n",
        "    net, im, fm = pnml_importer.apply(pnml_path)\n",
        "    return net, im, fm\n",
        "\n",
        "def playout_to_traces(net, im, fm, n_traces: int = 20000, max_trace_length: int = 400) -> List[List[str]]:\n",
        "    out: List[List[str]] = []\n",
        "    batch = 1000 \n",
        "    tries = 0\n",
        "\n",
        "    while len(out) < n_traces:\n",
        "        tries += 1\n",
        "        params = {\n",
        "            \"no_traces\": min(batch, n_traces - len(out)),\n",
        "            \"max_trace_length\": max_trace_length,\n",
        "            \"activity_key\": \"concept:name\",\n",
        "        }\n",
        "\n",
        "        elog = pn_playout.apply(\n",
        "            net, im, fm,\n",
        "            variant=pn_playout.Variants.BASIC_PLAYOUT,\n",
        "            parameters=params\n",
        "        )\n",
        "\n",
        "        for case in elog:\n",
        "            tr = []\n",
        "            for ev in case:\n",
        "                if \"concept:name\" in ev:\n",
        "                    tr.append(norm_act(ev[\"concept:name\"]))\n",
        "            if tr:\n",
        "                out.append(tr)\n",
        "\n",
        "        # safety: if PM4Py returns nothing, stop to avoid infinite loop\n",
        "        if tries > 100 and len(out) == 0:\n",
        "            break\n",
        "\n",
        "        if tries % 5 == 0:\n",
        "            print(f\"    [playout batching] collected={len(out)}/{n_traces}\")\n",
        "\n",
        "    print(f\"    [playout] requested={n_traces} | produced={len(out)} | max_len={max_trace_length}\")\n",
        "    return out[:n_traces]\n",
        "\n",
        "\n",
        "PAD = \"<PAD>\"\n",
        "UNK = \"<UNK>\"\n",
        "CLS = \"<CLS>\"\n",
        "\n",
        "def build_vocab_from_union(traces_a: List[List[str]], traces_b: List[List[str]]) -> Dict[str, int]:\n",
        "    vocab = {PAD: 0, UNK: 1, CLS: 2}\n",
        "    for tr in (traces_a + traces_b):\n",
        "        for a in tr:\n",
        "            if a not in vocab:\n",
        "                vocab[a] = len(vocab)\n",
        "    return vocab\n",
        "\n",
        "def tokenize_trace(trace: List[str], vocab: Dict[str, int], max_len: int) -> List[int]:\n",
        "    ids = [vocab[CLS]]\n",
        "    for a in trace[: max_len - 1]:\n",
        "        ids.append(vocab.get(a, vocab[UNK]))\n",
        "    return ids\n",
        "\n",
        "def corrupt_trace(trace: List[str], alphabet: List[str], noise: str, level: float, rng: random.Random) -> List[str]:\n",
        "    if not trace:\n",
        "        return trace\n",
        "    tr = trace.copy()\n",
        "    n_edits = max(1, int(round(level * len(tr))))\n",
        "    n_edits = min(n_edits, max(1, len(tr)))\n",
        "\n",
        "    if noise == \"delete\":\n",
        "        for _ in range(n_edits):\n",
        "            if not tr:\n",
        "                break\n",
        "            idx = rng.randrange(len(tr))\n",
        "            tr.pop(idx)\n",
        "        return tr if tr else trace[:1]\n",
        "\n",
        "    if noise == \"add\":\n",
        "        for _ in range(n_edits):\n",
        "            idx = rng.randrange(len(tr) + 1)\n",
        "            tr.insert(idx, rng.choice(alphabet))\n",
        "        return tr\n",
        "\n",
        "    if noise == \"replace\":\n",
        "        for _ in range(n_edits):\n",
        "            idx = rng.randrange(len(tr))\n",
        "            tr[idx] = rng.choice(alphabet)\n",
        "        return tr\n",
        "\n",
        "    if noise == \"swap\":\n",
        "        for _ in range(n_edits):\n",
        "            if len(tr) < 2:\n",
        "                break\n",
        "            i = rng.randrange(len(tr) - 1)\n",
        "            tr[i], tr[i + 1] = tr[i + 1], tr[i]\n",
        "        return tr\n",
        "\n",
        "    return tr\n",
        "\n",
        "def generate_antilog_mixed(positives: List[List[str]],\n",
        "                           alphabet: List[str],\n",
        "                           size: int,\n",
        "                           levels: Tuple[float, ...],\n",
        "                           noise_types: Tuple[str, ...],\n",
        "                           seed: int = 0) -> List[List[str]]:\n",
        "    rng = random.Random(SEED + seed)\n",
        "    negs: List[List[str]] = []\n",
        "    while len(negs) < size:\n",
        "        base = rng.choice(positives)\n",
        "        op = rng.choice(list(noise_types))\n",
        "        lvl = rng.choice(list(levels))\n",
        "        negs.append(corrupt_trace(base, alphabet, op, lvl, rng))\n",
        "    return negs\n",
        "\n",
        "\n",
        "class SeqDataset(Dataset):\n",
        "    def __init__(self, X: List[List[int]], y: List[int]):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "def collate_dynamic_pad(batch):\n",
        "    xs, ys = zip(*batch)\n",
        "    maxlen = max(len(x) for x in xs)\n",
        "    x_pad = torch.zeros(len(xs), maxlen, dtype=torch.long)\n",
        "    attn  = torch.zeros(len(xs), maxlen, dtype=torch.bool)\n",
        "    for i, x in enumerate(xs):\n",
        "        x_pad[i, :len(x)] = torch.tensor(x, dtype=torch.long)\n",
        "        attn[i, :len(x)] = True\n",
        "    y = torch.tensor(ys, dtype=torch.long)\n",
        "    return x_pad, attn, y\n",
        "\n",
        "class SinusoidalPositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, max_len: int = 512):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer(\"pe\", pe.unsqueeze(0))  # (1, T, D)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        T = x.size(1)\n",
        "        return x + self.pe[:, :T, :]\n",
        "\n",
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size: int,\n",
        "                 max_len: int,\n",
        "                 d_model: int = 128,\n",
        "                 n_heads: int = 4,\n",
        "                 n_layers: int = 2,\n",
        "                 d_ff: int = 512,\n",
        "                 dropout: float = 0.0,\n",
        "                 pad_idx: int = 0):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
        "        self.pos = SinusoidalPositionalEncoding(d_model=d_model, max_len=max_len)\n",
        "\n",
        "        enc_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=d_ff,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "            activation=\"relu\",   \n",
        "            norm_first=False    \n",
        "        )\n",
        "        self.enc = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
        "        self.fc = nn.Linear(d_model, 2)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, attn_mask_bool: torch.Tensor) -> torch.Tensor:\n",
        "        h = self.emb(x)\n",
        "        h = self.pos(h)\n",
        "        src_key_padding_mask = ~attn_mask_bool  # True means ignore\n",
        "        h = self.enc(h, src_key_padding_mask=src_key_padding_mask)\n",
        "        cls = h[:, 0, :]\n",
        "        return self.fc(cls)\n",
        "\n",
        "# Training + scoring\n",
        "@dataclass\n",
        "class TrainCfg:\n",
        "    lr: float = 3e-4\n",
        "    epochs: int = 14\n",
        "    batch_size: int = 16          \n",
        "    max_len: int = 192           \n",
        "    d_model: int = 128\n",
        "    n_heads: int = 4\n",
        "    n_layers: int = 2\n",
        "    d_ff: int = 512\n",
        "    dropout: float = 0.1        \n",
        "    n_playout: int = 30000      \n",
        "    max_trace_len_playout: int = 400\n",
        "    antilog_levels: Tuple[float, ...] = (0.10, 0.30, 0.50)  # mix easy/med/hard\n",
        "    anti_noise_types: Tuple[str, ...] = (\"add\",\"delete\",\"replace\",\"swap\")\n",
        "\n",
        "CFG = TrainCfg()\n",
        "\n",
        "def train_discriminator(model: nn.Module, dl: DataLoader, epochs: int, lr: float) -> None:\n",
        "    model.train()\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "    for ep in range(1, epochs + 1):\n",
        "        total_loss, total = 0.0, 0\n",
        "        for x, attn, y in dl:\n",
        "            x, attn, y = x.to(DEVICE), attn.to(DEVICE), y.to(DEVICE)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            logits = model(x, attn)\n",
        "            loss = F.cross_entropy(logits, y)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            total_loss += float(loss.item()) * y.size(0)\n",
        "            total += y.size(0)\n",
        "        print(f\"    epoch {ep:02d}/{epochs} | loss={total_loss/max(1,total):.4f}\")\n",
        "\n",
        "@torch.no_grad()\n",
        "def mean_pos_prob(model: nn.Module, X_tok: List[List[int]], bs: int = 256) -> float:\n",
        "    model.eval()\n",
        "    probs = []\n",
        "    for i in range(0, len(X_tok), bs):\n",
        "        chunk = X_tok[i:i+bs]\n",
        "        maxlen = max(len(x) for x in chunk)\n",
        "        x_pad = torch.zeros(len(chunk), maxlen, dtype=torch.long)\n",
        "        attn  = torch.zeros(len(chunk), maxlen, dtype=torch.bool)\n",
        "        for j, x in enumerate(chunk):\n",
        "            x_pad[j, :len(x)] = torch.tensor(x, dtype=torch.long)\n",
        "            attn[j, :len(x)] = True\n",
        "        x_pad, attn = x_pad.to(DEVICE), attn.to(DEVICE)\n",
        "        logits = model(x_pad, attn)\n",
        "        p = F.softmax(logits, dim=-1)[:, 1]\n",
        "        probs.append(p.detach().cpu().numpy())\n",
        "    return float(np.mean(np.concatenate(probs))) if probs else float(\"nan\")\n",
        "\n",
        "rows = []\n",
        "\n",
        "for ds_name in DATASETS:\n",
        "    log_path = os.path.join(LOGS_DIR, f\"{ds_name}.csv\")\n",
        "    if not os.path.isfile(log_path):\n",
        "        print(f\"[skip] Missing log CSV: {log_path}\")\n",
        "        continue\n",
        "    if ds_name not in models_map:\n",
        "        print(f\"[skip] No models found for dataset '{ds_name}' in {MODELS_DIR}\")\n",
        "        continue\n",
        "\n",
        "    print(\"\\n=========================================\")\n",
        "    print(\"Dataset:\", ds_name)\n",
        "    print(\"Log:\", log_path)\n",
        "\n",
        "    L = load_csv_trace_table(log_path)\n",
        "    print(f\"  Loaded event log traces: {len(L)}\")\n",
        "\n",
        "    model_levels = models_map[ds_name]\n",
        "    print(\"  Models found:\", [lvl for lvl, _ in model_levels])\n",
        "\n",
        "    for lvl, pnml_path in model_levels:\n",
        "        print(\"\\n-----------------------------------------\")\n",
        "        print(f\"  Model level {lvl}: {os.path.basename(pnml_path)}\")\n",
        "\n",
        "        # playout model log\n",
        "        net, im, fm = load_pnml_model(pnml_path)\n",
        "        LM = playout_to_traces(net, im, fm, n_traces=CFG.n_playout, max_trace_length=CFG.max_trace_len_playout)\n",
        "        print(f\"    Payout model traces: {len(LM)}\")\n",
        "\n",
        "        # vocab over union to avoid UNKs\n",
        "        vocab = build_vocab_from_union(L, LM)\n",
        "        vocab_size = len(vocab)\n",
        "\n",
        "        alphabet = sorted({a for tr in (L + LM) for a in tr})\n",
        "\n",
        "        LM_anti = generate_antilog_mixed(\n",
        "    LM, alphabet, size=len(LM),\n",
        "    levels=CFG.antilog_levels,\n",
        "    noise_types=CFG.anti_noise_types,\n",
        "    seed=1000 + lvl\n",
        ")\n",
        "        X_pos = [tokenize_trace(tr, vocab, CFG.max_len) for tr in LM]\n",
        "        X_neg = [tokenize_trace(tr, vocab, CFG.max_len) for tr in LM_anti]\n",
        "        X = X_pos + X_neg\n",
        "        y = [1]*len(X_pos) + [0]*len(X_neg)\n",
        "\n",
        "        dl_fit = DataLoader(\n",
        "            SeqDataset(X, y),\n",
        "            batch_size=CFG.batch_size,\n",
        "            shuffle=True,\n",
        "            collate_fn=collate_dynamic_pad,\n",
        "            num_workers=0\n",
        "        )\n",
        "\n",
        "        clf_fit = TransformerClassifier(\n",
        "            vocab_size=vocab_size,\n",
        "            max_len=CFG.max_len,\n",
        "            d_model=CFG.d_model,\n",
        "            n_heads=CFG.n_heads,\n",
        "            n_layers=CFG.n_layers,\n",
        "            d_ff=CFG.d_ff,\n",
        "            dropout=CFG.dropout,\n",
        "            pad_idx=0\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            xt = torch.randint(0, vocab_size, (2, 16), device=DEVICE)\n",
        "            at = torch.ones(2, 16, dtype=torch.bool, device=DEVICE)\n",
        "            _ = clf_fit(xt, at)\n",
        "\n",
        "        print(\"    Training fitness discriminator...\")\n",
        "        train_discriminator(clf_fit, dl_fit, epochs=CFG.epochs, lr=CFG.lr)\n",
        "        X_L = [tokenize_trace(tr, vocab, CFG.max_len) for tr in L]\n",
        "        fitness = mean_pos_prob(clf_fit, X_L)\n",
        "\n",
        "        L_anti = generate_antilog_mixed(\n",
        "    L, alphabet, size=len(L),\n",
        "    levels=CFG.antilog_levels,\n",
        "    noise_types=CFG.anti_noise_types,\n",
        "    seed=2000 + lvl\n",
        ")\n",
        "\n",
        "        X_pos2 = [tokenize_trace(tr, vocab, CFG.max_len) for tr in L]\n",
        "        X_neg2 = [tokenize_trace(tr, vocab, CFG.max_len) for tr in L_anti]\n",
        "        X2 = X_pos2 + X_neg2\n",
        "        y2 = [1]*len(X_pos2) + [0]*len(X_neg2)\n",
        "\n",
        "        dl_prec = DataLoader(\n",
        "            SeqDataset(X2, y2),\n",
        "            batch_size=CFG.batch_size,\n",
        "            shuffle=True,\n",
        "            collate_fn=collate_dynamic_pad,\n",
        "            num_workers=0\n",
        "        )\n",
        "\n",
        "        clf_prec = TransformerClassifier(\n",
        "            vocab_size=vocab_size,\n",
        "            max_len=CFG.max_len,\n",
        "            d_model=CFG.d_model,\n",
        "            n_heads=CFG.n_heads,\n",
        "            n_layers=CFG.n_layers,\n",
        "            d_ff=CFG.d_ff,\n",
        "            dropout=CFG.dropout,\n",
        "            pad_idx=0\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            xt = torch.randint(0, vocab_size, (2, 16), device=DEVICE)\n",
        "            at = torch.ones(2, 16, dtype=torch.bool, device=DEVICE)\n",
        "            _ = clf_prec(xt, at)\n",
        "\n",
        "        print(\"    Training precision discriminator...\")\n",
        "        train_discriminator(clf_prec, dl_prec, epochs=CFG.epochs, lr=CFG.lr)\n",
        "        X_LM = [tokenize_trace(tr, vocab, CFG.max_len) for tr in LM]\n",
        "        precision = mean_pos_prob(clf_prec, X_LM)\n",
        "\n",
        "        print(f\"    ==> Fitness={fitness:.4f} | Precision={precision:.4f}\")\n",
        "\n",
        "        rows.append({\n",
        "            \"dataset\": ds_name,\n",
        "            \"model_level\": int(lvl),\n",
        "            \"fitness\": float(fitness),\n",
        "            \"precision\": float(precision),\n",
        "            \"n_L\": int(len(L)),\n",
        "            \"n_LM\": int(len(LM)),\n",
        "            \"pnml\": os.path.basename(pnml_path)\n",
        "        })\n",
        "\n",
        "        del clf_fit, clf_prec, dl_fit, dl_prec, X, X2, y, y2, X_pos, X_neg, X_pos2, X_neg2\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "# Results dataframe + plots\n",
        "df = pd.DataFrame(rows)\n",
        "display(df.sort_values([\"dataset\", \"model_level\"]))\n",
        "\n",
        "out_csv = os.path.join(BASE, \"experiment3_transformer_results.csv\")\n",
        "os.makedirs(BASE, exist_ok=True)\n",
        "df.to_csv(out_csv, index=False)\n",
        "print(\"Saved:\", out_csv)\n",
        "\n",
        "# Plot per dataset: fitness + precision vs model level\n",
        "if not df.empty:\n",
        "    for ds_name in sorted(df[\"dataset\"].unique()):\n",
        "        sub = df[df[\"dataset\"] == ds_name].sort_values(\"model_level\")\n",
        "        fig = plt.figure(figsize=(7, 4))\n",
        "        plt.plot(sub[\"model_level\"], sub[\"fitness\"], marker=\"o\", label=\"Fitness\")\n",
        "        plt.plot(sub[\"model_level\"], sub[\"precision\"], marker=\"o\", label=\"Precision\")\n",
        "        plt.xlabel(\"Model level\")\n",
        "        plt.ylabel(\"Score\")\n",
        "        plt.title(ds_name)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.ylim(0.0, 1.02)\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "\n",
        "        out_pdf = os.path.join(BASE, f\"experiment3_{ds_name}_fitness_precision.pdf\")\n",
        "        out_png = os.path.join(BASE, f\"experiment3_{ds_name}_fitness_precision.png\")\n",
        "        plt.savefig(out_png, dpi=200)\n",
        "        plt.savefig(out_pdf, bbox_inches=\"tight\")\n",
        "        plt.show()\n",
        "        print(\"Saved:\", out_png)\n",
        "        print(\"Saved:\", out_pdf)\n",
        "else:\n",
        "    print(\"No results produced (check missing files / naming).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d7ecae3",
      "metadata": {},
      "source": [
        "Below code I used for testing generated logs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dffb3901",
      "metadata": {},
      "outputs": [],
      "source": [
        "A_L  = {a for tr in L for a in tr}\n",
        "A_LM = {a for tr in LM for a in tr}\n",
        "\n",
        "print(\"Activities in log:\", len(A_L))\n",
        "print(\"Activities in model log:\", len(A_LM))\n",
        "print(\"Overlap:\", len(A_L & A_LM), \"ratio:\", len(A_L & A_LM)/max(1,len(A_L)))\n",
        "print(\"Only in L:\", list(sorted(A_L - A_LM))[:20])\n",
        "print(\"Only in LM:\", list(sorted(A_LM - A_L))[:20])\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
